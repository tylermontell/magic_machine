This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-10-05T23:04:11.154Z

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
</notes>

<additional_info>

For more information about Repopack, visit: https://github.com/yamadashy/repopack
</additional_info>

</file_summary>

<repository_structure>
.github/
  workflows/
    repopack.yml
parsers/
  __init__.py
  etf_fund_parser.py
  general_parser.py
  htm_fund_extractor.py
  html_parser.py
  portfolio_basics_parser.py
  xls_parser.py
  xlsx_parser.py
test/
  config.py
  etf_fund_test.py
  port_test.py
  portfolio_annual.py
  portfolio_compare.py
  portfolio_parser.py
.gitignore
config.py
database.py
LICENSE
main.py
preprocess_repopack.py
repopack.config.json
trigger_folder.py
utils.py
</repository_structure>

<repository_files>
This section contains the contents of the repository's files.

<file path=".github/workflows/repopack.yml">
name: Repopack Automation

on:
  push:
    branches:
      - main

jobs:
  repopack:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checkout the repository
      - name: Checkout code
        uses: actions/checkout@v3  # Updated to use v3 to address Node.js version warnings

      # Step 2: Install Node.js (updated to version 20 to avoid future deprecations)
      - name: Install Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '20'

      # Step 3: Install Repopack globally
      - name: Install Repopack
        run: npm install -g repopack

      # Step 4: Verify repository access (for debugging purposes)
      - name: Verify Repository Access
        run: |
          git ls-remote https://github.com/tylermontell/magic_machine.git

      # Step 5: Run Repopack using the config file to ensure proper XML output
      - name: Run Repopack with Config
        run: |
          repopack --config repopack.config.json || exit 1
          echo "Repopack process completed."
          ls -l  # List the current directory to check for repopack-output.xml

      # Step 6: Check if repopack-output.xml exists
      - name: Check Repopack Output
        run: |
          if [ -f "repopack-output.xml" ]; then
            echo "Repopack output found!"
          else
            echo "Repopack output not generated."
            exit 1
          fi

      # Step 7: Run the preprocessing script only if repopack-output.xml exists
      - name: Run Preprocessing Script
        if: success()
        run: python preprocess_repopack.py

      # Step 8: Upload session summaries as artifacts
      - name: Upload Session Summaries
        uses: actions/upload-artifact@v3
        with:
          name: session-summaries
          path: session_summaries.txt

      # Step 9: Upload optimized repopack output as artifacts
      - name: Upload Optimized Output
        uses: actions/upload-artifact@v3
        with:
          name: optimized-repopack-output
          path: ./optimized-repopack-output.xml
</file>

<file path="parsers/__init__.py">
# __init__.py

from .xls_parser import parse_xls_file
from .xlsx_parser import parse_xlsx_file
from .html_parser import parse_html_file
</file>

<file path="parsers/etf_fund_parser.py">
import xml.etree.ElementTree as ET
import pandas as pd
import os
import re
from config import report_configs, namespaces, fund_expected_columns, etf_expected_columns


# --------------------------- Helper Functions --------------------------- #

def clean_text(text):
    return text.strip() if text else ''


def normalize_text(text):
    """Normalize text for consistent header matching."""
    return re.sub(r'\W+', '', text.lower().strip())


def append_portfolio_return_row(df, portfolio_return_row):
    """Append the portfolio return row to the DataFrame if it exists."""
    if len(portfolio_return_row) < len(df.columns):
        portfolio_return_row += [''] * (len(df.columns) - len(portfolio_return_row))
    elif len(portfolio_return_row) > len(df.columns):
        portfolio_return_row = portfolio_return_row[:len(df.columns)]
    
    portfolio_df = pd.DataFrame([portfolio_return_row], columns=df.columns)
    return pd.concat([df, portfolio_df], ignore_index=True)

def clean_data(df):
    # Remove rows where all values are empty, or consist of mostly commas/dashes
    df = df.replace(['-', '', 'nan'], '').dropna(how='all')
    
    # Remove rows with placeholder content (e.g., repeated commas)
    df = df[df.apply(lambda row: not all(cell in ['', '-'] for cell in row), axis=1)]
    
    # Drop rows that start with unwanted values like "S&P 500 TR USD" or "Morningstar"
    if 'Name' in df.columns:
        df = df[~df['Name'].str.contains("S&P 500 TR USD|Morningstar", na=False)]
    
    # Drop any duplicate rows just in case
    df = df.drop_duplicates()

    return df

def format_date_column(date_str):
    """
    Detects and formats date strings in 'YYYYMMDD' format to 'YYYY-MM-DD'.
    """
    # Check if the string is 8 digits long (matches 'YYYYMMDD' pattern)
    if re.match(r'^\d{8}$', str(date_str)):
        return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
    return date_str

# --------------------------- Main Processing Functions --------------------------- #

def get_report_type_from_descriptor(xml_file):
    """Determine report type by inspecting the XML file descriptors."""
    tree = ET.parse(xml_file)
    root = tree.getroot()
    ns = namespaces
    descriptors = root.findall('.//ss:Cell/ss:Data', namespaces=ns)
    found_descriptors = []

    for data in descriptors:
        if data.text:
            text = data.text.strip()
            found_descriptors.append(text)
            for report_type, config in report_configs.items():
                descriptor = config.get('descriptor')
                if descriptor and descriptor.strip().lower() == text.strip().lower():
                    print(f"Matched report type '{report_type}' for descriptor '{text}' in file {os.path.basename(xml_file)}")
                    return report_type
    print(f"No matching descriptor found in {os.path.basename(xml_file)}. Descriptors found: {found_descriptors}")
    return None


def parse_xml(xml_file, report_type):
    """Parse XML file based on report type and return parsed data."""
    config = report_configs[report_type]
    headers = config['headers']
    exclusions = config.get('exclusions', [])
    tree = ET.parse(xml_file)
    root = tree.getroot()
    ns = namespaces
    rows = root.findall('.//ss:Row', namespaces=ns)
    parsed_data = []
    header_found = False
    header_indices = []

    normalized_headers = [normalize_text(header) for header in headers]

    for idx, row in enumerate(rows):
        row_data = []
        cells = row.findall('.//ss:Cell', namespaces=ns)
        for cell in cells:
            data = cell.find('.//ss:Data', namespaces=ns)
            value = clean_text(data.text) if data is not None and data.text is not None else ''
            row_data.append(value)

        if not any(row_data):
            continue

        # Skip excluded rows
        if any(exclusion in ' '.join(row_data) for exclusion in exclusions):
            print(f"Skipping row due to exclusion: {row_data}")
            continue

        # Identify and capture headers
        if not header_found:
            normalized_row = [normalize_text(cell) for cell in row_data]
            matched_headers = [header for header in normalized_headers if header in normalized_row]
            match_threshold = 0.8
            if len(matched_headers) / len(normalized_headers) >= match_threshold:
                header_found = True
                print(f"Header found in {os.path.basename(xml_file)}: {row_data}")
                header_indices = [normalized_row.index(header) for header in matched_headers]
                continue
        else:
            # Align row data with headers
            row_aligned = [row_data[i] if i < len(row_data) else '' for i in header_indices]
            if len(row_aligned) != len(headers):
                print(f"Row length mismatch in {os.path.basename(xml_file)}: Expected {len(headers)}, got {len(row_aligned)}")
                continue
            parsed_data.append(row_aligned)

    if not header_found:
        print(f"No header found in {os.path.basename(xml_file)}")

    print(f"Headers: {headers}")
    print(f"Parsed Data (first 5 rows): {parsed_data[:5]}")

    return headers, parsed_data, None

def process_files(directory, report_type_filter=None):
    dataframes = {}
    
    for filename in os.listdir(directory):
        if filename.endswith('.xls'):
            filepath = os.path.join(directory, filename)
            report_type = get_report_type_from_descriptor(filepath)
            
            # Apply filtering if specific report types are being targeted
            if report_type_filter and report_type not in report_type_filter:
                continue

            if report_type is None:
                print(f"Could not determine report type for {filename}")
                continue

            headers, parsed_data, portfolio_return_row = parse_xml(filepath, report_type)
            if not parsed_data and not portfolio_return_row:
                print(f"No data found in {filename}")
                continue
            
            # Create a new DataFrame for the parsed data
            df = pd.DataFrame(parsed_data, columns=headers)
            df['report_type'] = report_type
            
            # Clean the DataFrame before further processing
            df = clean_data(df)
            
            # Collect DataFrames based on the type of report
            dataframes[report_type] = pd.concat([dataframes.get(report_type, pd.DataFrame()), df], ignore_index=True)
    
    return dataframes

def ensure_correct_column_alignment(df, expected_columns):
    """
    Ensure the dataframe columns align with the expected columns, filling missing columns with empty strings.
    Columns not in expected_columns will be removed.
    """
    # Reindex the dataframe with the expected columns. If some columns are missing, they will be added with empty values.
    df = df.reindex(columns=expected_columns, fill_value='')

    # Drop rows where all values are empty
    df = df.dropna(how='all')

    # Check if 'Name' and 'Ticker' columns exist before dropping duplicates
    if 'Name' in df.columns and 'Ticker' in df.columns:
        df = df.drop_duplicates(subset=['Name', 'Ticker'], keep='first')

    return df

# --------------------------- DataFrame Combination --------------------------- #

def combine_fund_dataframes(dataframes):
    """Combine all fund-related dataframes."""
    fund_dfs = [df for report_type, df in dataframes.items() if report_type.startswith('fund_')]
    funds_df = pd.concat(fund_dfs, ignore_index=True) if fund_dfs else pd.DataFrame()
    funds_df = clean_data(funds_df)
    return ensure_correct_column_alignment(funds_df, fund_expected_columns)


def combine_etf_dataframes(dataframes):
    """Combine all ETF-related dataframes."""
    etf_dfs = [df for report_type, df in dataframes.items() if report_type.startswith('etf_')]
    etfs_df = pd.concat(etf_dfs, ignore_index=True) if etf_dfs else pd.DataFrame()
    etfs_df = clean_data(etfs_df)
    return ensure_correct_column_alignment(etfs_df, etf_expected_columns)


# --------------------------- Main Function --------------------------- #

def remove_empty_rows_between_files(df):
    # Remove fully empty rows that may exist between separate files (Funds and ETFs)
    df = df.dropna(how='all')  # Drop rows where all values are empty
    return df

def process_fund_and_etf_dataframes(dataframes):
    # Process Fund Data
    funds_df = combine_fund_dataframes(dataframes)
    if not funds_df.empty:
        funds_df = remove_empty_rows_between_files(funds_df)
        funds_df.to_csv('funds_data.csv', index=False)
        print("Fund DataFrame has been cleaned, processed, and saved to funds_data.csv.")
    
    # Process ETF Data
    etfs_df = combine_etf_dataframes(dataframes)
    if not etfs_df.empty:
        etfs_df = remove_empty_rows_between_files(etfs_df)
        etfs_df.to_csv('etfs_data.csv', index=False)
        print("ETF DataFrame has been cleaned, processed, and saved to etfs_data.csv.")

# --------------------------- Script Execution --------------------------- #

if __name__ == "__main__":
    directory = '/Users/tylermontell/Projects/magic_machine_app/test/test_xls'
    dataframes = process_files(directory)
    process_fund_and_etf_dataframes(dataframes)
</file>

<file path="parsers/general_parser.py">
# general_parser.py

import os
import logging
from parsers.xls_parser import parse_xls_file
from parsers.xlsx_parser import parse_xlsx_file
from parsers.html_parser import parse_html_file  # Ensure this imports the correct function

logger = logging.getLogger(__name__)

def process_files(trigger_folder):
    """
    Process files in the given trigger folder.
    """
    file_paths = os.listdir(trigger_folder)
    logger.info(f"Found {len(file_paths)} files to process.")

    for file_name in file_paths:
        file_path = os.path.join(trigger_folder, file_name)

        # Skip system files like .DS_Store
        if file_name == '.DS_Store':
            continue
        
        logger.info(f"Processing file: {file_path}")
        try:
            if file_name.endswith('.xls'):
                data = parse_xls_file(file_path)
            elif file_name.endswith('.xlsx'):
                data = parse_xlsx_file(file_path)
            elif file_name.endswith('.html') or file_name.endswith('.htm'):
                # Process HTML files with the updated parse_html_file function
                data = parse_html_file(file_path)
            else:
                logger.warning(f"Unsupported file type: {file_name}")
                continue

            # Process or log the extracted data
            logger.info(f"Processed data: {data}")

        except Exception as e:
            logger.error(f"Error processing file {file_name}: {e}")
</file>

<file path="parsers/htm_fund_extractor.py">
import re
import pandas as pd
from bs4 import BeautifulSoup

# File paths
input_file = "/Users/tylermontell/Projects/magic_machine_app/data/trigger_folder/Tracking.htm"
output_file = "/Users/tylermontell/Projects/magic_machine_app/test/output.txt"

# Function to extract and clean relevant content from the HTML
def extract_relevant_content(html_content):
    # Use BeautifulSoup to parse the HTML content
    soup = BeautifulSoup(html_content, "html.parser")

    # Extract content between the specific classes
    start = soup.find(class_="core-account-notification")
    end = soup.find(class_="sidebar-layout-analytics-controls")
    
    # Extract the relevant section between start and end
    relevant_content = ""
    if start and end:
        for tag in start.find_all_next():
            if tag == end:
                break
            relevant_content += str(tag)

    return relevant_content

# Function to parse relevant fund and portfolio data
def parse_data(cleaned_html):
    soup = BeautifulSoup(cleaned_html, "html.parser")

    data = []

    # Extract both fund and portfolio data
    for section in soup.find_all(class_="allocation-title"):
        # Extract common fields
        short_name = section.find(class_="security-shortname").text.strip() if section.find(class_="security-shortname") else ''
        long_name = section.find(class_="security-longname").text.strip() if section.find(class_="security-longname") else ''
        
        # Extract stats only once per section
        risk_number = section.find_next("img", alt=re.compile("Risk Number"))["src"].split('r')[-1].split('.')[0] if section.find_next("img", alt=re.compile("Risk Number")) else ''
        worst_case = section.find_next("input", class_="worst-case")["placeholder"].replace('%', '').strip() if section.find_next("input", class_="worst-case") else ''
        best_case = section.find_next("input", class_="best-case")["placeholder"].replace('%', '').strip() if section.find_next("input", class_="best-case") else ''
        return_val = section.find_next("input", class_="analysis-return")["placeholder"].replace('%', '').strip() if section.find_next("input", class_="analysis-return") else ''
        stdev = section.find_next("input", class_="stdev")["placeholder"].replace('%', '').strip() if section.find_next("input", class_="stdev") else ''
        dividend = section.find_next("input", class_="ttm-dividend")["placeholder"].replace('%', '').strip() if section.find_next("input", class_="ttm-dividend") else ''
        fee = section.find_next("input", class_="annual-fee")["placeholder"].replace('%', '').strip() if section.find_next("input", class_="annual-fee") else ''
        
        # Append data as a single row
        if short_name or long_name:
            data.append([short_name, long_name, risk_number, worst_case, best_case, return_val, stdev, dividend, fee])

    return data

# Function to write data to a text file as a dataframe
def write_data_to_file(data, output_file):
    # Create DataFrame
    columns = ["ticker", "name", "r_risk_score", "low_6mo_return", "high_6mo_return", "pot_6mo_return", "vol", "est_dividend", "r_net_expense"]

    df = pd.DataFrame(data, columns=columns)

    # Remove any duplicate rows
    df = df.drop_duplicates()

    # Write to file
    with open(output_file, "w") as f:
        f.write(df.to_string(index=False))

# Main script execution
if __name__ == "__main__":
    with open(input_file, "r") as f:
        html_content = f.read()

    # Step 1: Extract relevant content
    cleaned_content = extract_relevant_content(html_content)

    # Step 2: Parse the data (both funds and portfolios)
    parsed_data = parse_data(cleaned_content)

    # Step 3: Write the parsed data to the output file
    write_data_to_file(parsed_data, output_file)

    print(f"Data successfully extracted and written to {output_file}")
</file>

<file path="parsers/html_parser.py">
# html_parser.py

import re
import pandas as pd
from bs4 import BeautifulSoup

# File paths
input_file = "/Users/tylermontell/Projects/magic_machine_app/data/trigger_folder/Tracking.htm"
output_file = "/Users/tylermontell/Projects/magic_machine_app/data/trigger_folder/output.txt"

# Function to extract and clean relevant content from the HTML
def extract_relevant_content(html_content):
    # Use BeautifulSoup to parse the HTML content
    soup = BeautifulSoup(html_content, "html.parser")

    # Extract content between the specific classes
    start = soup.find(class_="core-account-notification")
    end = soup.find(class_="sidebar-layout-analytics-controls")
    
    # Extract the relevant section between start and end
    relevant_content = ""
    if start and end:
        for tag in start.find_all_next():
            if tag == end:
                break
            relevant_content += str(tag)

    return relevant_content

# Function to parse relevant fund and portfolio data
def parse_data(cleaned_html):
    soup = BeautifulSoup(cleaned_html, "html.parser")

    data = []

    # Extract both fund and portfolio data
    for section in soup.find_all(class_="allocation-title"):
        # Extract common fields
        short_name = section.find(class_="security-shortname").text.strip() if section.find(class_="security-shortname") else ''
        long_name = section.find(class_="security-longname").text.strip() if section.find(class_="security-longname") else ''
        
        # Extract stats only once per section
        risk_number = section.find_next("img", alt=re.compile("Risk Number"))["src"].split('r')[-1].split('.')[0] if section.find_next("img", alt=re.compile("Risk Number")) else ''
        worst_case = section.find_next("input", class_="worst-case")["placeholder"].replace('%', '').strip() if section.find_next("input", class_="worst-case") else ''
        best_case = section.find_next("input", class_="best-case")["placeholder"].replace('%', '').strip() if section.find_next("input", class_="best-case") else ''
        return_val = section.find_next("input", class_="analysis-return")["placeholder"].replace('%', '').strip() if section.find_next("input", class_="analysis-return") else ''
        stdev = section.find_next("input", class_="stdev")["placeholder"].replace('%', '').strip() if section.find_next("input", class_="stdev") else ''
        dividend = section.find_next("input", class_="ttm-dividend")["placeholder"].replace('%', '').strip() if section.find_next("input", class_="ttm-dividend") else ''
        fee = section.find_next("input", class_="annual-fee")["placeholder"].replace('%', '').strip() if section.find_next("input", class_="annual-fee") else ''
        
        # Append data as a single row
        if short_name or long_name:
            data.append([short_name, long_name, risk_number, worst_case, best_case, return_val, stdev, dividend, fee])

    return data

# Function to write data to a text file as a dataframe
def write_data_to_file(data, output_file):
    # Create DataFrame
    columns = ["ticker", "name", "r_risk_score", "low_6mo_return", "high_6mo_return", "pot_6mo_return", "vol", "est_dividend", "r_net_expense"]

    df = pd.DataFrame(data, columns=columns)

    # Remove any duplicate rows
    df = df.drop_duplicates()

    # Write to file
    with open(output_file, "w") as f:
        f.write(df.to_string(index=False))

# Main script execution
if __name__ == "__main__":
    with open(input_file, "r") as f:
        html_content = f.read()

    # Step 1: Extract relevant content
    cleaned_content = extract_relevant_content(html_content)

    # Step 2: Parse the data (both funds and portfolios)
    parsed_data = parse_data(cleaned_content)

    # Step 3: Write the parsed data to the output file
    write_data_to_file(parsed_data, output_file)

    print(f"Data successfully extracted and written to {output_file}")
</file>

<file path="parsers/portfolio_basics_parser.py">
import xml.etree.ElementTree as ET
import pandas as pd
import os
import re
from config import namespaces, report_configs

# --------------------------- Helper Functions --------------------------- #

def clean_text(text):
    return text.strip() if text else ''

def normalize_text(text):
    """Normalize text for consistent header matching."""
    return re.sub(r'\W+', '', text.lower().strip())

def clean_data(df, exclusions):
    """Clean the dataframe by removing unwanted rows and applying exclusions."""
    # Remove rows where all values are empty, or consist of mostly commas/dashes/nan
    df = df.replace(['-', '', 'nan'], '').dropna(how='all')

    # Remove rows that match known exclusion terms, but don't exclude the header row
    exclusion_pattern = '|'.join([re.escape(exclusion) for exclusion in exclusions])
    df = df[~df.apply(lambda row: row.astype(str).str.contains(exclusion_pattern).any(), axis=1)]

    # Drop any duplicate rows just in case
    df = df.drop_duplicates()

    return df

# --------------------------- Main Processing Functions --------------------------- #

def parse_xml(xml_file, expected_columns, exclusions):
    """Parse XML file and return parsed data based on the expected headers."""
    tree = ET.parse(xml_file)
    root = tree.getroot()
    ns = namespaces
    rows = root.findall('.//ss:Row', namespaces=ns)
    parsed_data = []
    header_found = False
    header_indices = []

    normalized_headers = [normalize_text(header) for header in expected_columns]

    for idx, row in enumerate(rows):
        row_data = []
        cells = row.findall('.//ss:Cell', namespaces=ns)
        for cell in cells:
            data = cell.find('.//ss:Data', namespaces=ns)
            value = clean_text(data.text) if data is not None and data.text is not None else ''
            row_data.append(value)

        if not any(row_data):
            continue

        # Identify and capture headers
        if not header_found:
            normalized_row = [normalize_text(cell) for cell in row_data]
            matched_headers = [header for header in normalized_headers if header in normalized_row]
            match_threshold = 0.8
            if len(matched_headers) / len(normalized_headers) >= match_threshold:
                header_found = True
                print(f"Header found in {os.path.basename(xml_file)}: {row_data}")
                header_indices = [normalized_row.index(header) for header in matched_headers]
                continue
        else:
            # Align row data with headers
            row_aligned = [row_data[i] if i < len(row_data) else '' for i in header_indices]
            if len(row_aligned) != len(expected_columns):
                print(f"Row length mismatch in {os.path.basename(xml_file)}: Expected {len(expected_columns)}, got {len(row_aligned)}")
                continue
            parsed_data.append(row_aligned)

    if not header_found:
        print(f"No header found in {os.path.basename(xml_file)}")

    print(f"Headers: {expected_columns}")
    print(f"Parsed Data (first 5 rows): {parsed_data[:5]}")

    return expected_columns, parsed_data

def process_files(directory, report_type):
    """Process all files in the directory and parse data based on the report type."""
    dataframes = []
    config = report_configs[report_type]
    expected_columns = config['headers']
    exclusions = config.get('exclusions', [])

    for filename in os.listdir(directory):
        if filename.endswith('.xls'):
            filepath = os.path.join(directory, filename)
            headers, parsed_data = parse_xml(filepath, expected_columns, exclusions)
            if not parsed_data:
                print(f"No data found in {filename}")
                continue
            
            # Create a new DataFrame for the parsed data
            df = pd.DataFrame(parsed_data, columns=headers)

            # Clean the DataFrame before further processing
            df = clean_data(df, exclusions)
            
            # Add to the list of DataFrames
            dataframes.append(df)
    
    return dataframes

# --------------------------- DataFrame Combination --------------------------- #

def combine_dataframes(dataframes):
    """Combine all dataframes."""
    combined_df = pd.concat(dataframes, ignore_index=True) if dataframes else pd.DataFrame()
    return combined_df

# --------------------------- Main Function --------------------------- #

def process_and_save_data(directory, report_type):
    """Process all files and save the combined data to CSV."""
    # Process all files in the directory and combine them into a single DataFrame
    dataframes = process_files(directory, report_type)
    combined_df = combine_dataframes(dataframes)
    
    if not combined_df.empty:
        output_file = '/Users/tylermontell/Projects/magic_machine_app/test/basics_output.csv'
        combined_df.to_csv(output_file, index=False)
        print(f"Combined DataFrame has been cleaned, processed, and saved to {output_file}.")
    else:
        print("No data to process.")

# --------------------------- Script Execution --------------------------- #

if __name__ == "__main__":
    directory = '/Users/tylermontell/Projects/magic_machine_app/test/test_xls'
    process_and_save_data(directory, 'model_basics')
</file>

<file path="parsers/xls_parser.py">
# parsers/xls_parser.py

import logging
import xml.etree.ElementTree as ET
from config import NAMESPACES

logger = logging.getLogger(__name__)

def parse_xls_file(file_path):
    """
    Parse .xls files (SpreadsheetML XML format).
    """
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
        data = []

        # Iterate through the XML structure to extract data
        for row in root.findall('.//ss:Row', namespaces=NAMESPACES):
            row_data = []
            for cell in row.findall('.//ss:Data', namespaces=NAMESPACES):
                row_data.append(cell.text)
            data.append(row_data)
        
        return data

    except Exception as e:
        logger.error(f'Error parsing .xls file {file_path}: {e}')
        return []
</file>

<file path="parsers/xlsx_parser.py">
# parsers/xlsx_parser.py

import pandas as pd
import logging
import openpyxl
import os
from config import NAMESPACES

logger = logging.getLogger(__name__)

def save_to_excel(data, filename):
    """
    Save the parsed data to an Excel file.
    """
    # Define the output directory
    output_directory = '/Users/tylermontell/Projects/magic_machine_app/data/output'

    # Ensure the directory exists; if not, create it
    if not os.path.exists(output_directory):
        os.makedirs(output_directory)  # Create the directory if it doesn't exist

    # Define the full path for the output file
    file_path = os.path.join(output_directory, filename)

    # Save data to an Excel file using pandas
    df = pd.DataFrame(data)  # Assuming 'data' is in a form that can be converted to a DataFrame
    df.to_excel(file_path, index=False)

    # Log the successful save
    logger.info(f"Data saved to {file_path}")

def process_xlsx(file_path):
    """
    Parse and process an .xlsx file, then save to an output directory.
    """
    try:
        # Parse the xlsx file
        data = parse_xlsx_file(file_path)  # Parse and return data
        
        # Define output filename based on the input file
        filename = os.path.basename(file_path).replace('.xlsx', '_output.xlsx')

        # Call the save function to write data to an Excel file
        save_to_excel(data, filename)
        
    except Exception as e:
        logger.error(f"Error processing .xlsx file {file_path}: {e}")

def parse_xlsx_file(file_path):
    """
    Function to parse the .xlsx file and extract data.
    This should return the parsed data in a format that can be saved, like a list of dictionaries.
    """
    try:
        # Load the xlsx file
        data = pd.read_excel(file_path)
        
        # Convert the data to a dictionary or list of dictionaries if needed
        extracted_data = data.to_dict(orient='records')
        
        # Log the successful parsing
        logger.info(f"Parsed {len(extracted_data)} rows from {file_path}")
        
        return extracted_data
    
    except Exception as e:
        logger.error(f"Error parsing .xlsx file {file_path}: {e}")
        return []
</file>

<file path="test/config.py">
# config.py

# **General Configuration**

namespaces = {
    'ss': 'urn:schemas-microsoft-com:office:spreadsheet'
}

excluded_characters = ["©", "®"]

# **Report Configurations**

# The `report_configs` dictionary contains configurations for each report type.
# Each report type has:
# - 'headers': List of column headers expected in the report.
# - 'exclusions': List of phrases or words to exclude while parsing.
# - 'descriptor': Unique identifier found in the report to determine the report type.

report_configs = {
    # **Benchmark Reports**

    'bench_annual': {
        'descriptor': 'TM- BM Annual',
        'headers': [
            "Name", "Type", "Annual Return 2014", "Annual Return 2015",
            "Annual Return 2016", "Annual Return 2017", "Annual Return 2018",
            "Annual Return 2019", "Annual Return 2020", "Annual Return 2021",
            "Annual Return 2022", "Annual Return 2023"
        ],
        'exclusions': [
            "Benchmark Universe", "FINRA members", "Print Date", "Ranked by:",
            "ascending", "Benchmark:", "Aggregate:", "Average:", "©2024 Morningstar.",
            "2024 Morningstar"
        ],
    },
    'bench_compare': {
        'descriptor': 'TM- Benchmark Compare',
        'headers': [
            "Name", "Description", "Date", "Currency", "Constituents Applied",
            "Tot Ret 1 Mo (Qtr-end)", "Tot Ret 3 Mo (Qtr-end)", "Tot Ret YTD (Qtr-end)",
            "Tot Ret 12 Mo (Qtr-end)", "Tot Ret 3 Yr Annlzd (Qtr-end)",
            "Tot Ret 5 Yr Annlzd (Qtr-end)", "Tot Ret 10 Yr Annlzd (Qtr-end)",
            "Tot Ret 15 Yr Annlzd (Qtr-end)", "Standard Deviation 5 Yr",
            "Cyclical", "Sensitive", "Defensive", "Basic Materials", "Consumer Cyclical",
            "Financial Services", "Real Estate", "Communication Services", "Energy",
            "Industrials", "Technology", "Consumer Defensive", "Healthcare", "Utilities",
            "% Africa/ Middle East", "% Americas", "% North America", "% Latin America",
            "% Asia - Developed", "% Asia - Emerging", "% Australasia", "% United Kingdom",
            "% Europe - Developed", "% Europe - Emerging", "% Greater Asia", "% Greater Europe",
            "% Japan", "Market Maturity % Developed", "Market Maturity % Emerging",
            "Average Maturity", "Average Effective Duration", "Average Credit Quality",
            "Average Weighted Coupon", "AAA", "AA", "A", "BBB", "BB", "B", "Below B",
            "% Large Blend", "% Large Growth", "% Large Value", "% Mid Blend", "% Mid Growth",
            "% Mid Value", "% Small Blend", "% Small Growth", "% Small Value", "% Bonds Long",
            "% Bonds Short", "% Cash Long", "% Cash Short", "% US Stocks Long",
            "% US Stocks Short", "% Non-US Stocks Long", "% Non-US Stocks Short"
        ],
        'exclusions': [
            "Benchmark Universe", "FINRA members", "Print Date", "Ranked by:",
            "ascending", "Benchmark:", "Aggregate:", "Average:", "©2024 Morningstar.",
            "2024 Morningstar"
        ],
    },

    # **ETF Reports**

    'etf_annual': {
    'descriptor': 'TM- ETF Annual',
    'headers': [
        "Name", "Ticker", "Morningstar Category", "Mkt Annl Return 2014",
        "Mkt Annl Return 2015", "Mkt Annl Return 2016", "Mkt Annl Return 2017",
        "Mkt Annl Return 2018", "Mkt Annl Return 2019", "Mkt Annl Return 2020",
        "Mkt Annl Return 2021", "Mkt Annl Return 2022", "Mkt Annl Return 2023"
    ],
    'exclusions': [
        "US ETF Universe", "FINRA members:", "Print Date", "Ranked by:",
        "ascending", "Aggregate:", "Average:", "Benchmark:",
        "©2024 Morningstar.", "2024 Morningstar."
        ],
    },

    'etf_compare': {
        'descriptor': 'TM- ETF Compare Fund',
        'headers': [
            "Name", "Ticker", "Date (Mkt)", "Morningstar Category", "Prospectus  Benchmark",
            "Fund Objective", "Annual Report Net Expense  Ratio", "Mkt Tot Return 1 Mo (Qtr-end)",
            "Mkt Tot Return 3 Mo (Qtr-end)", "Mkt Tot Return YTD (Qtr-end)",
            "Mkt Tot Return 12 Mo (Qtr-end)", "Mkt Tot Return 3 Yr (Qtr-end)",
            "Mkt Tot Return 5 Yr (Qtr-end)", "Mkt Tot Return 10 Yr (Qtr-end)",
            "Mkt Tot Return 15 Yr (Qtr-end)", "Standard Deviation 5 Yr", "Cyclical",
            "Sensitive", "Defensive", "Basic Materials", "Consumer Cyclical",
            "Financial Services", "Real Estate", "Communication Services", "Energy",
            "Industrials", "Technology", "Consumer Defensive", "Healthcare", "Utilities",
            "% Africa/ Middle East", "% Americas", "% North America", "% Latin America",
            "% Asia - Developed", "% Asia - Emerging", "% Australasia", "% United Kingdom",
            "% Europe - Developed", "% Europe - Emerging", "% Greater Asia", "% Greater Europe",
            "% Japan", "Market Maturity % Developed", "Market Maturity % Emerging",
            "Average Maturity", "Average Effective Duration", "Average Credit Quality",
            "Average Weighted Coupon", "AAA", "AA", "A", "BBB", "BB", "B", "Below B",
            "Government %", "Corporate %", "Securitized %", "Municipal %",
            "Cash & Equivalents %", "Derivative %", "% Large Blend", "% Large Growth",
            "% Large Value", "% Mid Blend", "% Mid Growth", "% Mid Value", "% Small Blend",
            "% Small Growth", "% Small Value", "% Bonds Long", "% Bonds Short",
            "% Cash Long", "% Cash Short", "% US Stocks Long", "% US Stocks Short",
            "% Non-US Stocks Long", "% Non-US Stocks Short", "Portfolio Risk Score"
        ],
        'exclusions': [
            "Aggregate:", "Average:", "Benchmark:", "©2024 Morningstar", "2024 Morningstar"
        ],
    },
    'etf_model_compare': {
        'descriptor': 'TM- ETF Model Compare',
        'headers': [
            "Name", "Ticker", "Morningstar Category", "Date (Mkt)", "Mkt Tot Return 1 Mo (Qtr-end)",
            "Mkt Tot Return 3 Mo (Qtr-end)", "Mkt Tot Return YTD (Qtr-end)", "Mkt Tot Return 12 Mo (Qtr-end)",
            "Mkt Tot Return 3 Yr (Qtr-end)", "Mkt Tot Return 5 Yr (Qtr-end)", "Mkt Tot Return 10 Yr (Qtr-end)",
            "Mkt Tot Return 15 Yr (Qtr-end)", "Standard Deviation 5 Yr", "Average Maturity",
            "Average Effective Duration", "Average Weighted Coupon", "AAA", "AA", "A", "BBB", "BB", "B",
            "Below B", "NR/NA", "% Large Blend", "% Large Growth", "% Large Value", "% Mid Blend",
            "% Mid Growth", "% Mid Value", "% Small Blend", "% Small Growth", "% Small Value",
            "% Bonds Long", "% Bonds Short", "% Cash Long", "% Cash Short", "% US Stocks Long",
            "% US Stocks Short", "% Non-US Stocks Long", "% Non-US Stocks Short",
            "% Other/ Not Classified Long", "% Other/ Not Classified Short", "Cyclical",
            "Sensitive", "Defensive", "Basic Materials", "Consumer Cyclical", "Financial Services",
            "Real Estate", "Communication Services", "Energy", "Industrials", "Technology",
            "Consumer Defensive", "Healthcare", "Utilities", "Portfolio Risk Score"
        ],
        'exclusions': [
            "TM- Allocation ETF", "US ETF Universe", "Ranked by:", "Aggregate:", "Average:",
            "FINRA members", "Print Date", "Benchmark:", "S&P 500 TR USD", "©2024 Morningstar.",
            "2024 Morningstar."
        ],
    },

    # **Fund Reports**

    'fund_annual': {
        'descriptor': 'TM- MF Annual',
        'headers': [
            "Name", "Ticker", "Morningstar Category", "Annual Return 2014",
            "Annual Return 2015", "Annual Return 2016", "Annual Return 2017",
            "Annual Return 2018", "Annual Return 2019", "Annual Return 2020",
            "Annual Return 2021", "Annual Return 2022", "Annual Return 2023"
        ],
        'exclusions': [
            "US Mutual Fund Universe", "FINRA members:", "Print Date", "Ranked by:",
            "ascending", "Aggregate:", "Average:", "Benchmark:", "2024 Morningstar.",
            "©2024 Morningstar."
        ],
    },
    'fund_compare': {
        'descriptor': 'TM- Fund Compare',
        'headers': [
            "Name", "Ticker", "Date", "Morningstar Category", "Annual Report Net Expense  Ratio",
            "Tot Ret 1 Mo (Qtr-end)", "Tot Ret 3 Mo (Qtr-end)", "Tot Ret YTD (Qtr-end)",
            "Tot Ret 12 Mo (Qtr-end)", "Tot Ret 3 Yr Annlzd (Qtr-end)", "Tot Ret 5 Yr Annlzd (Qtr-end)",
            "Tot Ret 10 Yr Annlzd (Qtr-end)", "Tot Ret 15 Yr Annlzd (Qtr-end)", "Standard Deviation 5 Yr",
            "Cyclical", "Sensitive", "Defensive", "Basic Materials", "Consumer Cyclical",
            "Financial Services", "Real Estate", "Communication Services", "Energy", "Industrials",
            "Technology", "Consumer Defensive", "Healthcare", "Utilities", "% Africa/ Middle East",
            "% Americas", "% North America", "% Latin America", "% Asia - Developed", "% Asia - Emerging",
            "% Australasia", "% United Kingdom", "% Europe - Developed", "% Europe - Emerging",
            "% Greater Asia", "% Greater Europe", "% Japan", "Market Maturity % Developed",
            "Market Maturity % Emerging", "Average Maturity", "Average Effective Duration",
            "Average Credit Quality", "Average Weighted Coupon", "AAA", "AA", "A", "BBB", "BB", "B",
            "Below B", "Government %", "Corporate %", "Securitized %", "Municipal %",
            "Cash & Equivalents %", "Derivative %", "% Large Blend", "% Large Growth", "% Large Value",
            "% Mid Blend", "% Mid Growth", "% Mid Value", "% Small Blend", "% Small Growth", "% Small Value",
            "% Bonds Long", "% Bonds Short", "% Cash Long", "% Cash Short", "% US Stocks Long",
            "% US Stocks Short", "% Non-US Stocks Long", "% Non-US Stocks Short", "Portfolio Risk Score"
        ],
        'exclusions': [
            "US Mutual Fund Universe", "FINRA members:", "Print Date", "Ranked by:",
            "ascending", "Aggregate:", "Average:", "Benchmark:", "2024 Morningstar.",
            "©2024 Morningstar."
        ],
    },
    'fund_model_compare': {
        'descriptor': 'TM- Fund Model',
        'headers': [
            "Name", "Ticker", "Morningstar Category", "Date", "Tot Ret 1 Mo (Qtr-end)",
            "Tot Ret 3 Mo (Qtr-end)", "Tot Ret YTD (Qtr-end)", "Tot Ret 12 Mo (Qtr-end)",
            "Tot Ret 3 Yr Annlzd (Qtr-end)", "Tot Ret 5 Yr Annlzd (Qtr-end)",
            "Tot Ret 10 Yr Annlzd (Qtr-end)", "Tot Ret 15 Yr Annlzd (Qtr-end)",
            "Standard Deviation 5 Yr", "Average Maturity", "Average Effective Duration",
            "Average Weighted Coupon", "AAA", "AA", "A", "BBB", "BB", "B", "Below B",
            "NR/NA", "% Large Blend", "% Large Growth", "% Large Value", "% Mid Blend",
            "% Mid Growth", "% Mid Value", "% Small Blend", "% Small Growth", "% Small Value",
            "% Bonds Long", "% Bonds Short", "% Cash Long", "% Cash Short", "% US Stocks Long",
            "% US Stocks Short", "% Non-US Stocks Long", "% Non-US Stocks Short",
            "% Other/ Not Classified Long", "% Other/ Not Classified Short", "Cyclical",
            "Sensitive", "Defensive", "Basic Materials", "Consumer Cyclical", "Financial Services",
            "Real Estate", "Communication Services", "Energy", "Industrials", "Technology",
            "Consumer Defensive", "Healthcare", "Utilities", "Portfolio Risk Score"
        ],
        'exclusions': [
            "Aggregate:", "Average:", "Benchmark Universe", "FINRA members", "Print Date",
            "Ranked by:", "ascending", "Benchmark:"
        ],
    },

    # **Portfolio Reports**

    'model_basics': {
        'descriptor': 'Basics',
        'headers': [
            'Portfolio Name', 'Portfolio Risk Score', 'Portfolio Risk Score Date', 'Source', 
            'Editable', 'Service', 'Benchmark Name', 'Asset Allocation Model', 
            'Allocated By', 'Tracking Method', 'Created By', 'Date Created', 
            'Last Modified By', 'Last Modified Date/Time', 'Locked By', 'Locked Date/Time'
        ],
        'exclusions': [
            "FBA Mods", "Model Portfolios", "Basics", "FINRA members: For internal or institutional use only.",
            "Print Date", "Ranked by:", "ascending", "Portfolio Name", "©2024 Morningstar",
            "2024 Morningstar"
        ],
    },
    'model_compare': {
        'descriptor': 'Model Compare',
        'headers': [
            'Name', 'Return Date (current)', 'NAV Tot Ret 1 Mo (Qtr-end)', 'NAV Tot Ret 3 Mo (Qtr-end)',
            'NAV Tot Ret YTD (Qtr-end)', 'NAV Tot Ret 12 Mo (Qtr-end)', 'NAV Tot Ret 3 Yr Annlzd (Qtr-end)',
            'NAV Tot Ret 5 Yr Annlzd (Qtr-end)', 'NAV Tot Ret 10 Yr Annlzd (Qtr-end)',
            'NAV Tot Ret 15 Yr Annlzd (Qtr-end)', 'Mkt Standard Deviation 5 Yr', 'Average Maturity',
            'Average Effective Duration', 'Average Weighted Coupon', 'AAA', 'AA', 'A', 'BBB', 'BB', 'B',
            'Below B', 'NR/NA', '% Large Blend', '% Large Growth', '% Large Value', '% Mid Blend',
            '% Mid Growth', '% Mid Value', '% Small Blend', '% Small Growth', '% Small Value',
            '% Bonds Long', '% Bonds Short', '% Cash Long', '% Cash Short', '% US Stocks Long',
            '% US Stocks Short', '% Non-US Stocks Long', '% Non-US Stocks Short',
            '% Other/ Not Classified Long', '% Other/ Not Classified Short', 'Cyclical', 'Sensitive',
            'Defensive', 'Basic Materials', 'Consumer Cyclical', 'Financial Services', 'Real Estate',
            'Communication Services', 'Energy', 'Industrials', 'Technology', 'Consumer Defensive',
            'Healthcare', 'Utilities', 'Tot Ret +/-  Prim Bmrk 1 Mo (mo-end)',
            'Tot Ret +/-  Prim Bmrk 3 Mo (mo-end)', 'Tot Ret +/-  Prim Bmrk YTD (mo-end)',
            'Tot Ret +/-  Prim Bmrk 12 Mo (mo-end)', 'Tot Ret +/-  Prim Bmrk 3 Yr Annlzd (mo-end)',
            'Tot Ret +/-  Prim Bmrk 5 Yr Annlzd (mo-end)', 'Tot Ret +/-  Prim Bmrk 10 Yr Annlzd (mo-end)',
            'Tot Ret +/-  Prim Bmrk 15 Yr Annlzd (mo-end)', 'Alpha 3 Yr', 'Alpha 5 Yr', 'Alpha 10 Yr'
        ],
        'exclusions': [
            "FBA Mods", "Model Portfolios", "Basics", "FINRA members: For internal or institutional use only.",
            "Print Date", "Ranked by:", "ascending", "Portfolio Name", "©2024 Morningstar",
            "2024 Morningstar"
        ],
    },
    'model_annual': {
        'descriptor': 'Annual',
        'headers': [
            'Name', 'Morningstar Category', 'Mkt Annl Return  2014', 'Mkt Annl Return  2015',
            'Mkt Annl Return  2016', 'Mkt Annl Return  2017', 'Mkt Annl Return  2018',
            'Mkt Annl Return  2019', 'Mkt Annl Return  2020', 'Mkt Annl Return  2021',
            'Mkt Annl Return  2022', 'Mkt Annl Return  2023'
        ],
        'exclusions': [
            "FBA Mods", "Model Portfolios", "Basics", "FINRA members: For internal or institutional use only.",
            "Print Date", "Ranked by:", "ascending", "Portfolio Name", "©2024 Morningstar",
            "2024 Morningstar"
        ],
    },
}


fund_expected_columns = [
        'Name', 'Ticker', 'Morningstar Category', 'Annual Return 2014', 'Annual Return 2015', 'Annual Return 2016',
        'Annual Return 2017', 'Annual Return 2018', 'Annual Return 2019', 'Annual Return 2020', 'Annual Return 2021',
        'Annual Return 2022', 'Annual Return 2023', 'Date', 'Annual Report Net Expense  Ratio', 'Tot Ret 1 Mo (Qtr-end)', 
        'Tot Ret 3 Mo (Qtr-end)', 'Tot Ret YTD (Qtr-end)', 'Tot Ret 12 Mo (Qtr-end)', 'Tot Ret 3 Yr Annlzd (Qtr-end)', 
        'Tot Ret 5 Yr Annlzd (Qtr-end)', 'Tot Ret 10 Yr Annlzd (Qtr-end)', 'Tot Ret 15 Yr Annlzd (Qtr-end)', 
        'Standard Deviation 5 Yr', 'Cyclical', 'Sensitive', 'Defensive', "Basic Materials", "Consumer Cyclical", 
        "Financial Services", "Real Estate", "Communication Services", "Energy", "Industrials", "Technology", 
        "Consumer Defensive", "Healthcare", "Utilities", "% Africa/ Middle East", "% Americas", "% North America", 
        "% Latin America", "% Asia - Developed", "% Asia - Emerging", "% Australasia", "% United Kingdom", 
        "% Europe - Developed", "% Europe - Emerging", "% Greater Asia", "% Greater Europe", "% Japan", 
        "Market Maturity % Developed", "Market Maturity % Emerging", "Average Maturity", "Average Effective Duration", 
        "Average Credit Quality", "Average Weighted Coupon", "AAA", "AA", "A", "BBB", "BB", "B", "Below B", "NR/NA", 
        "Government %", "Corporate %", "Securitized %", "Municipal %", "Cash & Equivalents %", "Derivative %", 
        "% Large Blend", "% Large Growth", "% Large Value", "% Mid Blend", "% Mid Growth", "% Mid Value", "% Small Blend", 
        "% Small Growth", "% Small Value", "% Bonds Long", "% Bonds Short", "% Cash Long", "% Cash Short", 
        "% US Stocks Long", "% US Stocks Short", "% Non-US Stocks Long", "% Non-US Stocks Short", "Portfolio Risk Score"
]

# Expected columns for ETFs
etf_expected_columns = [
    "Name", "Ticker", "Date (Mkt)", "Morningstar Category", "Prospectus  Benchmark", "Fund Objective",
    "Annual Report Net Expense  Ratio", "Mkt Tot Return 1 Mo (Qtr-end)", "Mkt Tot Return 3 Mo (Qtr-end)", 
    "Mkt Tot Return YTD (Qtr-end)", "Mkt Tot Return 12 Mo (Qtr-end)", "Mkt Tot Return 3 Yr (Qtr-end)",
    "Mkt Tot Return 5 Yr (Qtr-end)", "Mkt Tot Return 10 Yr (Qtr-end)", "Mkt Tot Return 15 Yr (Qtr-end)", 
    "Standard Deviation 5 Yr", "Cyclical", "Sensitive", "Defensive", "Basic Materials", "Consumer Cyclical", 
    "Financial Services", "Real Estate", "Communication Services", "Energy", "Industrials", "Technology", 
    "Consumer Defensive", "Healthcare", "Utilities", "% Africa/ Middle East", "% Americas", "% North America", 
    "% Latin America", "% Asia - Developed", "% Asia - Emerging", "% Australasia", "% United Kingdom", 
    "% Europe - Developed", "% Europe - Emerging", "% Greater Asia", "% Greater Europe", "% Japan", 
    "Market Maturity % Developed", "Market Maturity % Emerging", "Average Maturity", "Average Effective Duration", 
    "Average Credit Quality", "Average Weighted Coupon", "AAA", "AA", "A", "BBB", "BB", "B", "Below B",
    "Government %", "Corporate %", "Securitized %", "Municipal %", "Cash & Equivalents %", "Derivative %", 
    "% Large Blend", "% Large Growth", "% Large Value", "% Mid Blend", "% Mid Growth", "% Mid Value", 
    "% Small Blend", "% Small Growth", "% Small Value", "% Bonds Long", "% Bonds Short", "% Cash Long", 
    "% Cash Short", "% US Stocks Long", "% US Stocks Short", "% Non-US Stocks Long", "% Non-US Stocks Short", 
    "Portfolio Risk Score", "Mkt Annl Return 2014", "Mkt Annl Return 2015", "Mkt Annl Return 2016",
    "Mkt Annl Return 2017", "Mkt Annl Return 2018", "Mkt Annl Return 2019", "Mkt Annl Return 2020", 
    "Mkt Annl Return 2021", "Mkt Annl Return 2022", "Mkt Annl Return 2023", "NR/NA", "% Other/ Not Classified Long", 
    "% Other/ Not Classified Short"
]
# **Additional Configuration**

# If you have any common exclusions or headers, you can define them here and reference in the report configs.

common_exclusions = ["Print Date", "Ranked by:", "ascending", "Aggregate:", "Average:", "©2024 Morningstar.", "2024 Morningstar"]
</file>

<file path="test/etf_fund_test.py">
import os
import pandas as pd
import xml.etree.ElementTree as ET
import re
from config import report_configs, namespaces

def get_report_type_from_descriptor(xml_file):
    tree = ET.parse(xml_file)
    root = tree.getroot()
    ns = namespaces
    descriptors = root.findall('.//ss:Cell/ss:Data', namespaces=ns)
    found_descriptors = []

    for data in descriptors:
        if data.text:
            text = data.text.strip()
            found_descriptors.append(text)
            for report_type, config in report_configs.items():
                descriptor = config.get('descriptor')
                if descriptor and descriptor.strip().lower() == text.strip().lower():
                    print(f"Matched report type '{report_type}' for descriptor '{text}' in file {os.path.basename(xml_file)}")
                    return report_type
    print(f"No matching descriptor found in {os.path.basename(xml_file)}. Descriptors found: {found_descriptors}")
    return None

def parse_xml(xml_file, report_type):
    config = report_configs[report_type]
    headers = config['headers']
    exclusions = config.get('exclusions', [])
    tree = ET.parse(xml_file)
    root = tree.getroot()
    ns = namespaces
    rows = root.findall('.//ss:Row', namespaces=ns)
    parsed_data = []
    header_found = False
    header_indices = []
    portfolio_return_row = None
    portfolio_name = "Unknown Portfolio"
    portfolio_date = "Unknown Date"

    # Extract portfolio name and date from the XML
    for data in root.findall('.//ss:Data', namespaces=ns):
        text_content = data.text.strip() if data.text else ""
        if text_content.startswith("Meridian") or text_content.startswith("G "):
            portfolio_name = text_content
        if re.match(r'\d{4}-\d{2}-\d{2}', text_content):
            portfolio_date = text_content

    for idx, row in enumerate(rows):
        row_data = []
        cells = row.findall('.//ss:Cell', namespaces=ns)
        for cell in cells:
            data = cell.find('.//ss:Data', namespaces=ns)
            value = data.text.strip() if data is not None and data.text else ''
            row_data.append(value)

        if not any(row_data):
            continue

        if any(exclusion in ' '.join(row_data) for exclusion in exclusions):
            print(f"Skipping row due to exclusion: {row_data}")
            continue

        if row_data[0] in ["Aggregate:", "Average:", "Benchmark:"]:
            if report_type.startswith('model_'):
                if idx + 1 < len(rows):
                    next_row = rows[idx + 1]
                    next_row_data = []
                    next_cells = next_row.findall('.//ss:Cell', namespaces=ns)
                    for cell in next_cells:
                        data = cell.find('.//ss:Data', namespaces=ns)
                        value = data.text.strip() if data else ''
                        next_row_data.append(value)

                    if len(next_row_data) > 0 and next_row_data[0] == "-":
                        next_row_data[0] = portfolio_name
                        next_row_data[1] = portfolio_date
                    while len(next_row_data) < len(headers):
                        next_row_data.append('')
                    if len(next_row_data) > len(headers):
                        next_row_data = next_row_data[:len(headers)]
                    portfolio_return_row = next_row_data
            continue

        if not header_found:
            normalized_row = [re.sub(r'\W+', '', cell.lower().strip()) for cell in row_data]
            normalized_headers = [re.sub(r'\W+', '', header.lower().strip()) for header in headers]
            match_threshold = 0.8
            matched_headers = [header for header in normalized_headers if header in normalized_row]
            if len(matched_headers) / len(normalized_headers) >= match_threshold:
                header_found = True
                print(f"Header found in {os.path.basename(xml_file)}")
                header_indices = [normalized_row.index(header) for header in matched_headers]
                continue
        elif header_found:
            row_aligned = [row_data[i] if i < len(row_data) else '' for i in header_indices]
            if len(row_aligned) != len(headers):
                print(f"Row length mismatch in {os.path.basename(xml_file)}: Expected {len(headers)}, got {len(row_aligned)}")
                continue
            parsed_data.append(row_aligned)

    if not header_found:
        print(f"No header found in {os.path.basename(xml_file)}")

    return headers, parsed_data, portfolio_return_row

def process_files(directory):
    dataframes = {}
    for filename in os.listdir(directory):
        if filename.endswith('.xls'):
            filepath = os.path.join(directory, filename)
            report_type = get_report_type_from_descriptor(filepath)

            if report_type not in ["model_compare", "model_annual"]:
                print(f"Skipping file {filename} because report type '{report_type}' is not 'model_compare' or 'model_annual'")
                continue

            headers, parsed_data, portfolio_return_row = parse_xml(filepath, report_type)
            if not parsed_data and not portfolio_return_row:
                print(f"No data found in {filename}")
                continue
            df = pd.DataFrame(parsed_data, columns=headers)
            df['report_type'] = report_type
            if portfolio_return_row:
                portfolio_return_row_with_type = portfolio_return_row + [report_type]
                portfolio_df = pd.DataFrame([portfolio_return_row_with_type], columns=df.columns)
                df = pd.concat([df, portfolio_df], ignore_index=True)
            if report_type in dataframes:
                dataframes[report_type] = pd.concat([dataframes[report_type], df], ignore_index=True)
            else:
                dataframes[report_type] = df
    return dataframes

def combine_dataframes(dataframes):
    model_compare_dfs = []
    model_annual_dfs = []

    for report_type, df in dataframes.items():
        if report_type == 'model_compare':
            model_compare_dfs.append(df)
        elif report_type == 'model_annual':
            model_annual_dfs.append(df)

    model_compare_df = pd.concat(model_compare_dfs, ignore_index=True) if model_compare_dfs else pd.DataFrame()
    model_annual_df = pd.concat(model_annual_dfs, ignore_index=True) if model_annual_dfs else pd.DataFrame()

    return model_compare_df, model_annual_df

if __name__ == "__main__":
    directory = '/Users/tylermontell/Projects/magic_machine_app/test/test_xls'
    dataframes = process_files(directory)
    model_compare_df, model_annual_df = combine_dataframes(dataframes)

    if not model_compare_df.empty:
        model_compare_df.to_csv('model_compare_data.csv', index=False)
    if not model_annual_df.empty:
        model_annual_df.to_csv('model_annual_data.csv', index=False)

    print("DataFrames have been saved to CSV files.")
</file>

<file path="test/port_test.py">
import os
from lxml import etree
import csv
from datetime import datetime
from config import report_configs, namespaces

# Step 1: Define the input directory and output file
input_directory = '/Users/tylermontell/Projects/magic_machine_app/test/test_xls'
output_file_path = '/Users/tylermontell/Projects/magic_machine_app/test/test_xls/filtered_data_output.csv'

# Step 2: Define XML namespaces
namespaces = {
    'ss': 'urn:schemas-microsoft-com:office:spreadsheet'
}

# Step 3: Helper function to parse date strings
def parse_date(date_str):
    try:
        return datetime.strptime(date_str, "%Y-%m-%d")
    except ValueError:
        return None

# Step 4: Function to clean text by removing excluded characters
def clean_text(text):
    for char in excluded_characters:
        text = text.replace(char, '')
    return text

# Step 5: Function to process each individual file
def process_file(file_path):
    with open(file_path, 'rb') as file:
        xml_data = file.read()
    
    # Parse the XML file using the etree module
    root = etree.fromstring(xml_data)

    # Initialize variables to capture key information
    portfolio_name = "Unknown Portfolio Name"  # Will be updated once the portfolio name is detected
    fund_rows = []  # List to store fund rows (e.g., those containing 'Vanguard')
    portfolio_return_row = None  # Will hold the row for the portfolio return
    most_recent_return_date_str = None  # Store the most recent date as a string
    most_recent_return_date_obj = None  # Store the most recent date as a datetime object

    # Detect the portfolio name (Meridian or G Portfolio)
    for data in root.xpath('//ss:Data', namespaces=namespaces):
        text_content = data.text.strip() if data.text else ''
        if text_content.startswith("Meridian") or text_content.startswith("G "):
            portfolio_name = text_content
            break  # Stop once the portfolio name is found

    # Extract fund rows and locate the "Aggregate:" row
    rows = root.xpath('//ss:Row', namespaces=namespaces)
    aggregate_found = False

    # Iterate through each row to gather data and locate the "Aggregate:" row
    for idx, data in enumerate(rows):
        row_data = [clean_text(d.text.strip() if d.text else '') for d in data.xpath('ss:Cell/ss:Data', namespaces=namespaces)]
        
        # Skip empty rows
        if not row_data:
            continue

        # Check if the row contains 'Vanguard' to identify fund rows
        if any("Vanguard" in cell for cell in row_data):
            # Find the return date from this row (2nd column)
            return_date_str = row_data[1] if len(row_data) > 1 else ''
            return_date_obj = parse_date(return_date_str)

            # Update the most recent return date if necessary
            if return_date_obj and (most_recent_return_date_obj is None or return_date_obj > most_recent_return_date_obj):
                most_recent_return_date_obj = return_date_obj  # For comparison
                most_recent_return_date_str = return_date_str  # Store as string
            
            # Append the row to the fund_rows list
            fund_rows.append(tuple(row_data))  # Store as tuple to avoid mutable data issues with duplicates

        # Find "Aggregate:" and capture the next row as the portfolio return row
        elif "Aggregate:" in row_data[0]:
            aggregate_found = True
            # Check if there's a next row to capture as the portfolio return row
            if idx + 1 < len(rows):
                next_row_data = rows[idx + 1].xpath('ss:Cell/ss:Data', namespaces=namespaces)
                portfolio_return_row = [clean_text(d.text.strip() if d.text else '') for d in next_row_data]
                
                # Replace `"-"` with the actual portfolio name
                if len(portfolio_return_row) > 0 and portfolio_return_row[0] == "-":
                    portfolio_return_row[0] = portfolio_name
                
                # If there are fewer columns than expected, fill in empty columns
                while len(portfolio_return_row) < len(portfolio_annual_headers):
                    portfolio_return_row.append('')
                portfolio_return_row = tuple(portfolio_return_row)  # Store as tuple to ensure uniformity in duplicates checking

            break  # Stop once we find the "Aggregate:" row and the next row
    
    # Return the fund rows and portfolio return row
    return fund_rows, portfolio_return_row

# Step 6: Process all files in the input directory
all_rows = set()  # Use a set to automatically handle duplicate rows

for filename in os.listdir(input_directory):
    if filename.endswith(".xls"):
        file_path = os.path.join(input_directory, filename)
        print(f"Processing: {file_path}")
        fund_rows, portfolio_return_row = process_file(file_path)

        # Add all fund rows to the set (to ensure no duplicates)
        all_rows.update(fund_rows)

        # Add the portfolio return row (if it exists)
        if portfolio_return_row:
            all_rows.add(portfolio_return_row)

# Step 7: Write the output data to a CSV file
with open(output_file_path, mode='w', newline='') as csv_file:
    writer = csv.writer(csv_file)

    # Write the headers
    writer.writerow(portfolio_annual_headers)

    # Write all the rows from the set, which now contains no duplicates
    for row in all_rows:
        writer.writerow(row)

# Output completion message
print(f"All files processed and consolidated into: {output_file_path}")
</file>

<file path="test/portfolio_annual.py">
import os
from lxml import etree
import csv
from datetime import datetime
from config import report_configs, namespaces, excluded_characters

# Step 1: Define the input directory and output file
input_directory = '/Users/tylermontell/Projects/magic_machine_app/test/ann_xls'
output_file_path = '/Users/tylermontell/Projects/magic_machine_app/test/test_xls/filtered_data_output.csv'

# Step 2: Define XML namespaces
namespaces = {
    'ss': 'urn:schemas-microsoft-com:office:spreadsheet'
}

# Step 3: Helper function to parse date strings
def parse_date(date_str):
    try:
        return datetime.strptime(date_str, "%Y-%m-%d")
    except ValueError:
        return None

# Step 4: Function to clean text by removing excluded characters
def clean_text(text):
    for char in excluded_characters:
        text = text.replace(char, '')
    return text

# Step 5: Function to process each individual file
def process_file(file_path):
    with open(file_path, 'rb') as file:
        xml_data = file.read()
    
    # Parse the XML file using the etree module
    root = etree.fromstring(xml_data)

    # Initialize variables to capture key information
    portfolio_name = "Unknown Portfolio Name"  # Will be updated once the portfolio name is detected
    fund_rows = []  # List to store fund rows (e.g., those containing 'Vanguard')
    portfolio_return_row = None  # Will hold the row for the portfolio return
    most_recent_return_date_str = None  # Store the most recent date as a string
    most_recent_return_date_obj = None  # Store the most recent date as a datetime object

    # Detect the portfolio name (Meridian or G Portfolio)
    for data in root.xpath('//ss:Data', namespaces=namespaces):
        text_content = data.text.strip() if data.text else ''
        if text_content.startswith("Meridian") or text_content.startswith("G "):
            portfolio_name = text_content
            break  # Stop once the portfolio name is found

    # Extract fund rows and locate the "Aggregate:" row
    rows = root.xpath('//ss:Row', namespaces=namespaces)
    aggregate_found = False

    # Iterate through each row to gather data and locate the "Aggregate:" row
    for idx, row in enumerate(rows):
        row_data = [clean_text(d.text.strip() if d.text else '') for d in row.xpath('ss:Cell/ss:Data', namespaces=namespaces)]
        
        # Skip empty rows
        if not row_data:
            continue

        # Check if the row contains 'Vanguard' to identify fund rows
        if any("Vanguard" in cell for cell in row_data):
            # Find the return date from this row (2nd column)
            return_date_str = row_data[1] if len(row_data) > 1 else ''
            return_date_obj = parse_date(return_date_str)

            # Update the most recent return date if necessary
            if return_date_obj and (most_recent_return_date_obj is None or return_date_obj > most_recent_return_date_obj):
                most_recent_return_date_obj = return_date_obj  # For comparison
                most_recent_return_date_str = return_date_str  # Store as string
            
            # Append the row to the fund_rows list
            fund_rows.append(tuple(row_data))  # Store as tuple to avoid mutable data issues with duplicates

        # Find "Aggregate:" and capture the next row as the portfolio return row
        elif "Aggregate:" in row_data[0]:
            aggregate_found = True
            # Check if there's a next row to capture as the portfolio return row
            if idx + 1 < len(rows):
                next_row_data = rows[idx + 1].xpath('ss:Cell/ss:Data', namespaces=namespaces)
                portfolio_return_row = [clean_text(d.text.strip() if d.text else '') for d in next_row_data]
                
                # Replace `"-"` with the actual portfolio name
                if len(portfolio_return_row) > 0 and portfolio_return_row[0] == "-":
                    portfolio_return_row[0] = portfolio_name
                
                # Ensure correct number of columns
                while len(portfolio_return_row) < len(report_configs['model_annual']['headers']):
                    portfolio_return_row.append('')
                portfolio_return_row = tuple(portfolio_return_row)  # Store as tuple for uniformity

            break  # Stop once we find the "Aggregate:" row and the next row
    
    # Return the fund rows and portfolio return row
    return fund_rows, portfolio_return_row

# Step 6: Process all files in the input directory
all_rows = set()  # Use a set to automatically handle duplicate rows

for filename in os.listdir(input_directory):
    if filename.endswith(".xls"):
        file_path = os.path.join(input_directory, filename)
        print(f"Processing: {file_path}")
        fund_rows, portfolio_return_row = process_file(file_path)

        # Add all fund rows to the set (to ensure no duplicates)
        all_rows.update(fund_rows)

        # Add the portfolio return row (if it exists)
        if portfolio_return_row:
            all_rows.add(portfolio_return_row)

# Step 7: Write the output data to a CSV file
with open(output_file_path, mode='w', newline='') as csv_file:
    writer = csv.writer(csv_file)

    # Write the headers
    writer.writerow(report_configs['model_annual']['headers'])

    # Write all the rows from the set, which now contains no duplicates
    for row in all_rows:
        writer.writerow(row)

# Output completion message
print(f"All files processed and consolidated into: {output_file_path}")
</file>

<file path="test/portfolio_compare.py">
import os
from lxml import etree
import csv
import pandas as pd
from datetime import datetime
from config import portfolio_compare_headers  # Importing the portfolio headers from the config file

# Define the directories
input_directory = '/Users/tylermontell/Projects/Magic_Machine/mstar_portfolio/Test'
output_file_path = '/Users/tylermontell/Projects/Magic_Machine/mstar_portfolio/Outputs/consolidated_portfolio_data.csv'

# Create output directory if it doesn't exist
os.makedirs(os.path.dirname(output_file_path), exist_ok=True)

# Define XML namespaces
namespaces = {
    'ss': 'urn:schemas-microsoft-com:office:spreadsheet'
}

# Helper function to parse date strings
def parse_date(date_str):
    try:
        return datetime.strptime(date_str, "%Y-%m-%d")
    except ValueError:
        return None

# List to hold all the rows across all files
all_data_rows = []

# Function to process each file
def process_file(input_file_path):
    print(f"Processing: {input_file_path}")
    
    with open(input_file_path, 'rb') as file:
        xml_data = file.read()

    # Parse the XML file
    root = etree.fromstring(xml_data)

    # Initialize variables to capture key information
    portfolio_name = None  # Set to None initially
    fund_rows = []
    portfolio_return_row = None
    most_recent_return_date_str = None
    most_recent_return_date_obj = None

    # Step 1: Detect the portfolio name (Meridian or G Portfolio)
    for data in root.xpath('//ss:Data', namespaces=namespaces):
        text_content = data.text.strip() if data.text else ''
        if text_content.startswith("Meridian") or text_content.startswith("G "):
            portfolio_name = text_content
            break

    # Step 2: If no valid portfolio name is found, skip processing
    if not portfolio_name:
        print(f"Skipping file {input_file_path} due to missing portfolio name.")
        return  # Exit the function without adding any data for this file

    # Step 3: Extract fund rows and locate the "Aggregate:" row
    rows = root.xpath('//ss:Row', namespaces=namespaces)
    aggregate_found = False

    for idx, data in enumerate(rows):
        row_data = [d.text.strip() if d.text else '' for d in data.xpath('ss:Cell/ss:Data', namespaces=namespaces)]
        
        # Skip empty rows
        if not row_data:
            continue

        # Check if the row contains 'Vanguard' to identify fund rows
        if any("Vanguard" in cell for cell in row_data):
            # Find the return date from this row (2nd column)
            return_date_str = row_data[1] if len(row_data) > 1 else ''
            return_date_obj = parse_date(return_date_str)

            # Update the most recent return date if necessary
            if return_date_obj and (most_recent_return_date_obj is None or return_date_obj > most_recent_return_date_obj):
                most_recent_return_date_obj = return_date_obj  # For comparison
                most_recent_return_date_str = return_date_str  # Store as string
            
            # Append the row to the fund_rows list
            fund_rows.append(row_data)

        # Step 4: Find "Aggregate:" and capture the next row as the portfolio return row
        elif "Aggregate:" in row_data[0]:
            aggregate_found = True
            # Check if there's a next row to capture as the portfolio return row
            if idx + 1 < len(rows):
                next_row_data = rows[idx + 1].xpath('ss:Cell/ss:Data', namespaces=namespaces)
                portfolio_return_row = [d.text.strip() if d.text else '' for d in next_row_data]
            break  # Stop once we find the "Aggregate:" row and the next row

    # Step 5: Add the portfolio name and most recent return date to the portfolio return row
    if portfolio_return_row:
        # Ensure the portfolio_return_row has at least 2 columns
        if len(portfolio_return_row) < 2:
            portfolio_return_row.extend([''] * (2 - len(portfolio_return_row)))
        
        # Update the first two columns with the portfolio name and return date
        portfolio_return_row[0] = portfolio_name
        portfolio_return_row[1] = most_recent_return_date_str if most_recent_return_date_str else "N/A"
    else:
        # Fallback in case "Aggregate:" row wasn't found, create an empty row
        portfolio_return_row = [portfolio_name, most_recent_return_date_str if most_recent_return_date_str else "N/A"] + \
                               [""] * (len(portfolio_headers) - 2)

    # Step 6: Append all fund rows and portfolio return row to the combined list
    all_data_rows.extend(fund_rows)  # Add all component fund rows
    all_data_rows.append(portfolio_return_row)  # Add the portfolio return row

# Step 7: Iterate through all files in the input directory
for filename in os.listdir(input_directory):
    if filename.endswith(".xls"):
        input_file = os.path.join(input_directory, filename)
        process_file(input_file)

# Step 8: Remove duplicates from the combined data
# Convert to a DataFrame for easier duplicate removal
df = pd.DataFrame(all_data_rows, columns=portfolio_compare_headers)

# Drop duplicate rows based on 'Name' and 'Return Date (current)' columns
df.drop_duplicates(subset=['Name', 'Return Date (current)'], keep='first', inplace=True)

# Step 9: Write the final DataFrame to a single consolidated CSV file
df.to_csv(output_file_path, index=False)

print(f"All files processed and consolidated into: {output_file_path}")
</file>

<file path="test/portfolio_parser.py">
import pandas as pd
import os
from lxml import etree as ET
import re

# Namespace for XML
namespaces = {'ss': 'urn:schemas-microsoft-com:office:spreadsheet'}

# Clean and prepare the text
def clean_text(text):
    return text.strip() if text else ''

# Detect portfolio name based on specific patterns
def detect_portfolio_name(row):
    for cell in row:
        if cell.startswith("G ") or cell.startswith("Meridian"):
            return cell
    return None

# Combine portfolio names with their data rows
def combine_portfolio_and_data(rows):
    combined_rows = []
    skip_next = False

    for idx, row in enumerate(rows):
        if not skip_next:
            portfolio_name = detect_portfolio_name(row)
            if portfolio_name and idx + 1 < len(rows):
                combined_row = [portfolio_name] + rows[idx + 1][1:]
                combined_rows.append(combined_row)
                skip_next = True  # Skip the next row as it's been merged
            else:
                combined_rows.append(row)
        else:
            skip_next = False  # Reset skip
    return combined_rows

# Process the XML rows
def process_rows(root):
    rows = root.xpath('//ss:Row', namespaces=namespaces)
    extracted_rows = []

    for row in rows:
        row_data = [clean_text(cell.text) for cell in row.xpath('ss:Cell/ss:Data', namespaces=namespaces)]
        if row_data and not re.match(r"Aggregate:", row_data[0]):
            extracted_rows.append(row_data)

    return extracted_rows

# Adjust row to match expected column count (either truncate or pad with empty values)
def adjust_row_length(row, expected_length):
    if len(row) > expected_length:
        return row[:expected_length]  # Truncate to expected length
    elif len(row) < expected_length:
        return row + [''] * (expected_length - len(row))  # Pad with empty strings
    return row

# Process files and create CSV
def process_files(directory):
    all_data = []

    for filename in os.listdir(directory):
        if filename.endswith('.xls'):
            filepath = os.path.join(directory, filename)
            tree = ET.parse(filepath)
            root = tree.getroot()

            # Extract and clean rows
            extracted_rows = process_rows(root)
            combined_rows = combine_portfolio_and_data(extracted_rows)

            # Append to final data, adjusting row length to match header count
            for row in combined_rows:
                all_data.append(adjust_row_length(row, 12))  # Adjust rows to 12 columns

    # Create a DataFrame and save to CSV
    headers = ['Name', 'Morningstar Category', 'Mkt Annl Return 2014', 'Mkt Annl Return 2015', 'Mkt Annl Return 2016', 'Mkt Annl Return 2017', 'Mkt Annl Return 2018', 'Mkt Annl Return 2019', 'Mkt Annl Return 2020', 'Mkt Annl Return 2021', 'Mkt Annl Return 2022', 'Mkt Annl Return 2023']
    df = pd.DataFrame(all_data, columns=headers)
    df.to_csv('cleaned_portfolio_data.csv', index=False)

# Run the process
if __name__ == "__main__":
    directory = '/Users/tylermontell/Projects/magic_machine_app/test/test_xls'
    process_files(directory)
</file>

<file path=".gitignore">
# macOS generated files
.DS_Store
.AppleDouble
.LSOverride

# Icon must end with two \r
Icon
# Thumbnails
._*

# Files that might appear on external disks
.Spotlight-V100
.Trashes

# macOS cache files
.DS_Store?
._*
.Spotlight-V100
.Trashes
**/.DS_Store
**/._*

# macOS metadata files
._*

# macOS disk image files
.DMG
.sparseimage

# VSC specific settings and extensions
.vscode/*
!.vscode/settings.json
!.vscode/tasks.json
!.vscode/launch.json
!.vscode/extensions.json

# Node modules and logs (if using Node.js)
node_modules/
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*

# Compiled files
*.o
*.out
*.class

# Application data
*.pid
*.seed
*.seed[]
*.pid.lock
venv/

# Temporary files created by your text editor or IDE
*.swp
*.swo
*~
.idea/
*.sublime-project
*.sublime-workspace

# Ignore build directories
build/
dist/
out/

# Ignore any temp files generated by system or applications
temp/
tmp/

# General macOS hidden files
.fseventsd
.hfs+
.com.apple.timemachine.donotpresent
.apdisk
venv_magic_machine/
</file>

<file path="config.py">
# config.py

import os

# General Configurations
TRIGGER_FOLDER = os.path.join(os.getcwd(), 'data', 'trigger_folder')
PROCESSED_FOLDER = os.path.join(os.getcwd(), 'data', 'processed')
SAMPLE_OUTPUT_FOLDER = os.path.join(os.getcwd(), 'data', 'samples')
LOG_FOLDER = os.path.join(os.getcwd(), 'logs')

# Namespaces for XML parsing
NAMESPACES = {'ss': 'urn:schemas-microsoft-com:office:spreadsheet'}

# Database configuration
DATABASE_URI = 'sqlite:///mutual_funds.db'

# Logging configuration
LOG_FILE = "/Users/tylermontell/Projects/magic_machine_app/logs/application.log"

# Excluded characters and terms
EXCLUDED_CHARACTERS = ["©", "™", "®"]
EXCLUDED_TERMS = [
    "TM-", "Benchmark Universe", "FINRA members", "Print Date",
    "Ranked by:", "ascending", "Benchmark:", "Aggregate:", "Average:",
    "©2024 Morningstar.", "2024 Morningstar", "©2023 Morningstar.", "2023 Morningstar", "©2022 Morningstar.", "2022 Morningstar", "All Rights Reserved", "©2021 Morningstar.", "2021 Morningstar", "©2020 Morningstar.", "2020 Morningstar"
]

# Header mappings
HEADER_MAPPINGS = {

"Mid Blend": "%_mid_blend",
"Mid Growth": "%_mid_growth",
"Mid Value": "%_mid_value",
"% Africa/ Middle East": "%_africa_middle_east",
"Africa/ Middle East": "%_africa_middle_east",
"% Americas": "%_americas",
"Americas": "%_americas",
"% Asia - Developed": "%_asia_developed",
"Asia - Developed": "%_asia_developed",
"% Asia - Emerging": "%_asia_emerging",
"Asia - Emerging": "%_asia_emerging",
"% Australasia": "%_australasia",
"Australasia": "%_australasia",
"Basic Materials": "%_basic_materials",
"% Bonds Long": "%_bonds_long",
"Bonds Long": "%_bonds_long",
"% Bonds Short": "%_bonds_short",
"Bonds Short": "%_bonds_short",
"Cash & Equivalents": "%_cash",
"Cash & Equivalents %": "%_cash",
"% Cash Long": "%_cash_long",
"Cash Long": "%_cash_long",
"% Cash Short": "%_cash_short",
"Cash Short": "%_cash_short",
"Communication Services": "%_communication_services",
"Communications": "%_communication_services",
"Consumer Cyclical": "%_consumer_cyclical",
"Consumer Defensive": "%_consumer_defensive",
"Corporate": "%_corporate",
"Corporate %": "%_corporate",
"Cyclical": "%_cyclical",
"Defensive": "%_defensive",
"Derivative": "%_derivative",
"Derivative %": "%_derivative",
"Energy": "%_energy",
"% Europe - Developed": "%_europe_developed",
"Europe - Developed": "%_europe_developed",
"% Europe - Emerging": "%_europe_emerging",
"Europe - Emerging": "%_europe_emerging",
"% Financial Services": "%_fin_services",
"Financial Services": "%_fin_services",
"Government": "%_government",
"Government %": "%_government",
"% Greater Asia": "%_greater_asia",
"Greater Asia": "%_greater_asia",
"% Greater Europe": "%_greater_europe",
"Greater Europe": "%_greater_europe",
"Healthcare": "%_healthcare",
"% Japan": "%_japan",
"Japan": "%_japan",
"% Large Blend": "%_large_blend",
"Large Blend": "%_large_blend",
"% Large Growth": "%_large_growth",
"Large Growth": "%_large_growth",
"% Large Value": "%_large_value",
"Large Value": "%_large_value",
"% Latin America": "%_latin_america",
"Latin America": "%_latin_america",
"% Mid Blend": "%_mid_blend",
"% Mid Growth": "%_mid_growth",
"% Mid Value": "%_mid_value",
"Market Maturity % Developed": "%_mkt_maturity_dev",
"Market Maturity % Developed ": "%_mkt_maturity_dev",
"Maturity % Developed ": "%_mkt_maturity_dev",
"Market Maturity % Emerging": "%_mkt_maturity_emerging",
"Market Maturity % Emerging ": "%_mkt_maturity_emerging",
"Maturity % Emerging ": "%_mkt_maturity_emerging",
"Municipal": "%_municipal",
"Municipal %": "%_municipal",
"% Non-US Stocks Long": "%_non_us_stocks_long",
" Non-US Stocks Short": "%_non_us_stocks_short",
"% Non-US Stocks Short": "%_non_us_stocks_short",
"% North America": "%_north_america",
"North America": "%_north_america",
"% Other/ Not Classified Long": "%_other_not_class_long",
"Not Classified Long": "%_other_not_class_long",
"% Other/ Not Classified Short": "%_other_not_class_short",
"Not Classified Short": "%_other_not_class_short",
"Real Estate": "%_real_estate",
"Securitized": "%_securitized",
"Securitized %": "%_securitized",
"Sensitive": "%_sensitive",
"% Small Blend": "%_small_blend",
"Small Blend": "%_small_blend",
"% Small Growth": "%_small_growth",
"Small Growth": "%_small_growth",
"% Small Value": "%_small_value",
"Small Value": "%_small_value",
"% United Kingdom": "%_united_kingdom",
"United Kingdom": "%_united_kingdom",
"% US Stocks Long": "%_us_stocks_long",
"US Stocks Long": "%_us_stocks_long",
"% US Stocks Short": "%_us_stocks_short",
"US Stocks Short": "%_us_stocks_short",
"Utilities": "%_utilities",
"A": "a",
"AA": "aa",
"AAA": "aaa",
"Allocated By": "alloc_by",
"Alpha 10 Yr": "alpha_10yr",
"Alpha10": "alpha_10yr",
"Alpha 3 Yr": "alpha_3yr",
"Alpha3": "alpha_3yr",
"Alpha 5 Yr": "alpha_5yr",
"Alpha5": "alpha_5yr",
"2010": "ar_10",
"2011": "ar_11",
"2012": "ar_12",
"2013": "ar_13",
"2014": "ar_14",
"Annual Return 2014": "ar_14",
"Mkt Annl Return 2014": "ar_14",
"2015": "ar_15",
"Annual Return 2015": "ar_15",
"Mkt Annl Return 2015": "ar_15",
"2016": "ar_16",
"Annual Return 2016": "ar_16",
"Mkt Annl Return 2016": "ar_16",
"2017": "ar_17",
"Annual Return 2017": "ar_17",
"Mkt Annl Return 2017": "ar_17",
"2018": "ar_18",
"Annual Return 2018": "ar_18",
"Mkt Annl Return 2018": "ar_18",
"2019": "ar_19",
"Annual Return 2019": "ar_19",
"Mkt Annl Return 2019": "ar_19",
"2020": "ar_20",
"Annual Return 2020": "ar_20",
"Mkt Annl Return 2020": "ar_20",
"2021": "ar_21",
"Annual Return 2021": "ar_21",
"Mkt Annl Return 2021": "ar_21",
"2022": "ar_22",
"Annual Return 2022": "ar_22",
"Mkt Annl Return 2022": "ar_22",
"2023": "ar_23",
"Annual Return 2023": "ar_23",
"Mkt Annl Return 2023": "ar_23",
"Annual Report Net Expense  Ratio": "ar_net_expense",
"Asset Allocation Model": "asset_alloc_model",
"Average Coupon": "avg_coupon",
"Average Credit Quality": "avg_credit_quality",
"Average Duration": "avg_duration",
"Average Effective Duration": "avg_duration",
"Average Maturity": "avg_maturity",
"Average Quality": "avg_quality",
"Average Weighted Coupon": "avg_weighted_coupon",
"B": "b",
"BB": "bb",
"BBB": "bbb",
"Below B": "below_b",
"Benchmark": "benchmark",
"Benchmark Name": "benchmark",
"BLANK": "blank",
"Constituents  Applied": "constituents_applied",
"Created By": "created_by",
"Currency": "currency",
"Date": "date",
"Date (Mkt)": "date",
"Date Created": "date",
"Return Date (current)": "date",
"Return Date(c)": "date",
"Description": "description",
"Editable": "editable",
"Div": "est_dividend",
"Dividend": "est_dividend",
"Estimated Dividend": "m_est_dividend",
"Fund Objective": "fund_obj",
"Group": "group",
"High PR": "high_6mo_return",
"hP12R": "high_6mo_return",
"Best Case": "high_6mo_return",
"L Score": "l_score",
"Last Modified By": "last_mod_by",
"Last Modified Date/Time": "last_mod_date",
"Locked By": "locked_by",
"Locked Date/Time": "locked_date",
"Low PR": "low_6mo_return",
"lP12R": "low_6mo_return",
"Worst Case": "low_6mo_return",
"M*Risk": "m_risk_score",
"MPRS": "m_risk_score",
"Portfolio  Risk Score ": "m_risk_score",
"Portfolio Risk Score": "m_risk_score",
"Portfolio Risk  Score Date": "m_risk_score_date",
"Portfolio Risk Score Date": "m_risk_score_date",
"Category": "m_cat",
"Morningstar Category": "m_cat",
"Name": "name",
"Name ": "name",
"Portfolio Name": "name",
"security-longname": "name",
"Net Expense": "net_expense",
"Expense Ratio": "r_net_expense",
"Non-US Stocks Long": "non_us_stocks_long",
"NR/NA": "nr_na",
"Potential 12 Mo Return": "pot_12mo_return",
"Potential 6 Mo Return": "pot_6mo_return",
"P6R": "pot_6mo_return",
"Return": "pot_6mo_return",
"Projected Return": "pot_6mo_return",
"Primary Prospectus Benchmark": "prospectus_benchmark",
"Prospectus  Benchmark": "prospectus_benchmark",
"Risk Score": "r_risk_score",
"Risk Number": "r_risk_score",
"Service": "service",
"Shortname": "shortname",
"Source": "source",
"Mkt Standard Deviation 5 Yr ": "standard_dev_5yr",
"Standard Deviation 5 ": "standard_dev_5yr",
"Standard Deviation 5 Yr": "standard_dev_5yr",
"T Score": "t_score",
"Ticker": "ticker",
"security-shortname": "ticker",
"10 Year": "tot_ret_10yr",
"10Y": "tot_ret_10yr",
"10Ym": "tot_ret_10yr",
"Mkt Tot Ret 10 Yr Annlzd (mo-end)": "tot_ret_10yr",
"Mkt Tot Return 10 Yr (mo-end)": "tot_ret_10yr",
"Mkt Tot Return 10 Yr (Qtr-end)": "tot_ret_10yr",
"NAV Tot Ret 10 Yr Annlzd (Qtr-end)": "tot_ret_10yr",
"Tot Ret 10 Yr Annlzd (mo-end)": "tot_ret_10yr",
"Tot Ret 10 Yr Annlzd (Qtr-end)": "tot_ret_10yr",
"12 Month": "tot_ret_12mo",
"12M": "tot_ret_12mo",
"12Mc": "tot_ret_12mo",
"Mkt Tot Ret 12 Mo (Current)": "tot_ret_12mo",
"Mkt Tot Ret 12 Mo (mo-end)": "tot_ret_12mo",
"Mkt Tot Return 12 Mo (Qtr-end)": "tot_ret_12mo",
"NAV Tot Ret 12 Mo (Qtr-end)": "tot_ret_12mo",
"Tot Ret 12 Mo (current)": "tot_ret_12mo",
"Tot Ret 12 Mo (mo-end)": "tot_ret_12mo",
"Tot Ret 12 Mo (Qtr-end)": "tot_ret_12mo",
"15 Year": "tot_ret_15yr",
"Mkt Tot Ret 15 Yr Annlzd (mo-end)": "tot_ret_15yr",
"Mkt Tot Return 15 Yr (Qtr-end)": "tot_ret_15yr",
"NAV Tot Ret 15 Yr Annlzd (Qtr-end)": "tot_ret_15yr",
"Tot Ret 15 Yr Annlzd (mo-end)": "tot_ret_15yr",
"Tot Ret 15 Yr Annlzd (Qtr-end)": "tot_ret_15yr",
"30 Day ": "tot_ret_1mo",
"4Wc": "tot_ret_1mo",
"4Wk": "tot_ret_1mo",
"Mkt Tot Ret 1 Mo (mo-end)": "tot_ret_1mo",
"Mkt Tot Ret 4 Wk (current)": "tot_ret_1mo",
"Mkt Tot Return 1 Mo (Qtr-end)": "tot_ret_1mo",
"NAV Tot Ret 1 Mo (Qtr-end)": "tot_ret_1mo",
"Tot Ret 1 Mo (mo-end)": "tot_ret_1mo",
"Tot Ret 1 Mo (Qtr-end)": "tot_ret_1mo",
"Tot Ret 4 Wk (current)": "tot_ret_1mo",
"90 Day": "tot_ret_3mo",
"90 Day(c)": "tot_ret_3mo",
"90Dc": "tot_ret_3mo",
"Mkt Tot Ret 3 Mo (Current)": "tot_ret_3mo",
"Mkt Tot Ret 3 Mo (current) ": "tot_ret_3mo",
"Mkt Tot Ret 3 Mo (mo-end)": "tot_ret_3mo",
"Mkt Tot Return 3 Mo (Qtr-end)": "tot_ret_3mo",
"NAV Tot Ret 3 Mo (Qtr-end)": "tot_ret_3mo",
"Tot Ret 3 Mo (current)": "tot_ret_3mo",
"Tot Ret 3 Mo (mo-end)": "tot_ret_3mo",
"Tot Ret 3 Mo (Qtr-end)": "tot_ret_3mo",
"3 Year": "tot_ret_3yr",
"3Y": "tot_ret_3yr",
"3Ym": "tot_ret_3yr",
"Mkt Tot Ret 3 Yr Annlzd (mo-end)": "tot_ret_3yr",
"Mkt Tot Return 3 Yr (mo-end)": "tot_ret_3yr",
"Mkt Tot Return 3 Yr (Qtr-end)": "tot_ret_3yr",
"NAV Tot Ret 3 Yr Annlzd (Qtr-end)": "tot_ret_3yr",
"Tot Ret 3 Yr Annlzd (mo-end)": "tot_ret_3yr",
"Tot Ret 3 Yr Annlzd (Qtr-end)": "tot_ret_3yr",
"5 Year": "tot_ret_5yr",
"5Y": "tot_ret_5yr",
"5Ym": "tot_ret_5yr",
"Mkt Tot Ret 5 Yr Annlzd (mo-end)": "tot_ret_5yr",
"Mkt Tot Return 5 Yr (mo-end)": "tot_ret_5yr",
"Mkt Tot Return 5 Yr (Qtr-end)": "tot_ret_5yr",
"NAV Tot Ret 5 Yr Annlzd (Qtr-end)": "tot_ret_5yr",
"Tot Ret 5 Yr Annlzd (mo-end)": "tot_ret_5yr",
"Tot Ret 5 Yr Annlzd (Qtr-end)": "tot_ret_5yr",
"OB10": "tot_ret_diff_bmrk_10yr",
"Tot Ret +/-  Prim Bmrk 10 Yr Annlzd (mo-end)": "tot_ret_diff_bmrk_10yr",
"OB12": "tot_ret_diff_bmrk_12mo",
"Tot Ret +/-  Prim Bmrk 12 Mo (mo-end)": "tot_ret_diff_bmrk_12mo",
"OB15": "tot_ret_diff_bmrk_15yr",
"Tot Ret +/-  Prim Bmrk 15 Yr Annlzd (mo-end)": "tot_ret_diff_bmrk_15yr",
"OB30": "tot_ret_diff_bmrk_1mo",
"Tot Ret +/-  Prim Bmrk 1 Mo (mo-end)": "tot_ret_diff_bmrk_1mo",
"OB90": "tot_ret_diff_bmrk_3mo",
"Tot Ret +/-  Prim Bmrk 3 Mo (mo-end)": "tot_ret_diff_bmrk_3mo",
"OB3": "tot_ret_diff_bmrk_3yr",
"Tot Ret +/-  Prim Bmrk 3 Yr Annlzd (mo-end)": "tot_ret_diff_bmrk_3yr",
"OB5": "tot_ret_diff_bmrk_5yr",
"Tot Ret +/-  Prim Bmrk 5 Yr Annlzd (mo-end)": "tot_ret_diff_bmrk_5yr",
"OBYTD": "tot_ret_diff_bmrk_ytd",
"Tot Ret +/-  Prim Bmrk YTD (mo-end)": "tot_ret_diff_bmrk_ytd",
"Mkt Tot Ret YTD (Current)": "tot_ret_ytd",
"Mkt Tot Ret YTD (mo-end)": "tot_ret_ytd",
"Mkt Tot Return YTD (Qtr-end)": "tot_ret_ytd",
"NAV Tot Ret YTD (Qtr-end)": "tot_ret_ytd",
"Tot Ret YTD (current)": "tot_ret_ytd",
"Tot Ret YTD (mo-end)": "tot_ret_ytd",
"Tot Ret YTD (Qtr-end)": "tot_ret_ytd",
"YTD": "tot_ret_ytd",
"YTDc": "tot_ret_ytd",
"Tracking Method": "tracking_method",
"Vol": "vol",
"Stdev": "vol"
}
</file>

<file path="database.py">
# database.py

from sqlalchemy import create_engine, Column, Integer, String, Float, Date, MetaData
from sqlalchemy.orm import sessionmaker, declarative_base
from datetime import datetime
import logging
from config import DATABASE_URI

logger = logging.getLogger(__name__)

Base = declarative_base()

class MutualFund(Base):
    __tablename__ = 'mutual_funds'
    id = Column(Integer, primary_key=True)
    fund_name = Column(String)
    ticker_symbol = Column(String)
    date = Column(Date)
    nav_tot_ret_3_mo = Column(Float)

def get_engine():
    return create_engine(DATABASE_URI)

def create_tables():
    engine = get_engine()
    Base.metadata.create_all(engine)
    logger.info('Database tables created.')

def insert_data(data_list):
    engine = get_engine()
    Session = sessionmaker(bind=engine)
    session = Session()
    for data in data_list:
        try:
            # Map data to MutualFund object
            fund = MutualFund(
                fund_name=data.get('Fund Name'),
                ticker_symbol=data.get('Ticker Symbol'),
                date=datetime.strptime(data.get('Date'), '%Y-%m-%d') if data.get('Date') else None,
                nav_tot_ret_3_mo=float(data.get('90Dq')) if data.get('90Dq') else None,
                # Map other fields
            )
            session.merge(fund)
        except Exception as e:
            logger.error(f'Error inserting data: {e}')
            session.rollback()
    session.commit()
    logger.info('Data insertion completed.')
</file>

<file path="LICENSE">
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
</file>

<file path="main.py">
# main.py

import argparse
import logging
from parsers.general_parser import process_files  # Corrected import path
from config import LOG_FILE, TRIGGER_FOLDER  # Assuming these paths are defined in config

def configure_logging(log_file):
    """
    Configures logging to log to both a file and the console with reduced verbosity.
    """
    logging.basicConfig(
        level=logging.WARNING,  # Reduce verbosity by only logging WARNING and above
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, mode='w'),  # Logs to a file, overwrites each time
            logging.StreamHandler()  # Logs to the console
        ]
    )

def main():
    # Set up argument parser
    parser = argparse.ArgumentParser(description='Mutual Fund Data Parser')
    parser.add_argument('--process', action='store_true', help='Process new files in the trigger folder.')
    args = parser.parse_args()

    # Configure logging
    configure_logging(LOG_FILE)
    logger = logging.getLogger(__name__)

    if args.process:
        logger.info('Starting the file processing.')
        try:
            process_files(TRIGGER_FOLDER)  # Pass the trigger folder to the file processor
            logger.info('File processing completed successfully.')
        except Exception as e:
            logger.error(f'An error occurred: {e}')
    else:
        logger.info('No action specified. Use --process to process new files.')
        parser.print_help()

if __name__ == '__main__':
    main()
</file>

<file path="preprocess_repopack.py">
import os
import re

# Path to the repopack output file
REPOPACK_FILE = "repopack-output.xml"
OUTPUT_FILE = "optimized-repopack-output.xml"
IGNORE_PATTERNS = ['*.log', '*.test', 'tmp/', 'node_modules/', 'venv/', '__pycache__']

def should_ignore(file_path):
    """Check if the file matches any ignore patterns."""
    for pattern in IGNORE_PATTERNS:
        if re.search(pattern, file_path):
            return True
    return False

def extract_summaries(lines):
    """Extract summaries and return them as a list."""
    summary_section = []
    for line in lines:
        if "<summary>" in line:
            summary_section.append(line)
    return summary_section

def reorder_critical_sections(lines):
    """Reorder critical sections like summaries and config files at the top."""
    critical_sections = []
    remaining_lines = []
    for line in lines:
        if "<summary>" in line or "README" in line or "config.py" in line:
            critical_sections.append(line)
        else:
            remaining_lines.append(line)
    return critical_sections + remaining_lines

def preprocess_repopack_output():
    """Main function to preprocess repopack output file."""
    with open(REPOPACK_FILE, 'r') as f:
        lines = f.readlines()

    # Step 1: Filter out ignored files
    cleaned_lines = [line for line in lines if not should_ignore(line)]

    # Step 2: Extract summaries
    summaries = extract_summaries(cleaned_lines)

    # Step 3: Reorder critical sections
    reordered_lines = reorder_critical_sections(cleaned_lines)

    # Step 4: Write the optimized output
    with open(OUTPUT_FILE, 'w') as f:
        f.write("<!-- Session Summaries -->\n")
        f.writelines(summaries)
        f.write("<!-- Remaining Content -->\n")
        f.writelines(reordered_lines)

    print(f"Optimized repopack output written to {OUTPUT_FILE}")

if __name__ == "__main__":
    preprocess_repopack_output()
</file>

<file path="repopack.config.json">
{
    "output": {
      "filePath": "repopack-output.xml",  
      "style": "xml"  
    },
    "remote": {
      "url": "https://github.com/tylermontell/magic_machine.git",
      "branch": "main" 
    },
    "includeFiles": [
      "README.md",
      "src/",
      "docs/"
    ]
  }
</file>

<file path="trigger_folder.py">
# trigger_folder.py

import os
import re
import logging
from config import TRIGGER_FOLDER, NAMESPACES

# Set up logging
logger = logging.getLogger(__name__)

def standardize_file_name(file_name):
    """
    Standardize the file name by:
    - Converting to lowercase
    - Replacing spaces with underscores
    - Removing special characters
    """
    # Convert to lowercase
    file_name = file_name.lower()
    
    # Replace spaces with underscores
    file_name = file_name.replace(' ', '_')
    
    # Remove any characters that are not alphanumeric, underscores, or periods (for file extensions)
    file_name = re.sub(r'[^\w\._]', '', file_name)
    
    return file_name

def rename_files_in_trigger_folder():
    """
    Rename files in the trigger_folder to computer-readable, standardized names.
    """
    for file_name in os.listdir(TRIGGER_FOLDER):
        file_path = os.path.join(TRIGGER_FOLDER, file_name)
        
        # Ignore directories, only rename files
        if os.path.isfile(file_path):
            new_file_name = standardize_file_name(file_name)
            new_file_path = os.path.join(TRIGGER_FOLDER, new_file_name)
            
            # Rename the file if the new name is different from the old name
            if new_file_name != file_name:
                try:
                    os.rename(file_path, new_file_path)
                    logger.info(f"Renamed {file_name} to {new_file_name}")
                except Exception as e:
                    logger.error(f"Error renaming file {file_name}: {e}")

# Call this function at the start of your file processing workflow
rename_files_in_trigger_folder()
</file>

<file path="utils.py">
# utils.py

import logging
import sys
import pandas as pd
from config import EXCLUDED_CHARACTERS, EXCLUDED_TERMS, NAMESPACES

# Define logger for utils.py
logger = logging.getLogger(__name__)

def clean_data(data):
    """
    Clean the data by removing rows that contain any of the excluded terms
    or characters and ensuring that no excluded characters are present.
    """
    cleaned_data = []

    for row in data:
        # Ensure that any None values are handled safely
        cleaned_row = [cell if cell is not None else '' for cell in row]

        # Remove excluded characters from each cell
        for char in EXCLUDED_CHARACTERS:
            cleaned_row = [cell.replace(char, '') for cell in cleaned_row]

        # Only keep rows where no cell contains any excluded terms
        if not any(term in cell for term in EXCLUDED_TERMS for cell in cleaned_row):
            # Keep rows that have at least one non-excluded value
            if any(cell.strip() not in ['', '-'] for cell in cleaned_row):
                cleaned_data.append(cleaned_row)

    return cleaned_data

def map_headers(data, header_mappings):
    if not data:
        return []
    # Map headers
    original_headers = data[0]
    mapped_headers = [header_mappings.get(header, header) for header in original_headers]
    # Replace headers with mapped headers
    data[0] = mapped_headers
    # Convert list of lists to list of dictionaries
    data_dicts = []
    for row in data[1:]:
        row_dict = dict(zip(mapped_headers, row))
        data_dicts.append(row_dict)
    return data_dicts

def write_sample_output(data, sample_file):
    """
    Write the parsed data to a text file for manual review and display as a DataFrame.
    """
    try:
        if not data:
            logger.warning(f'No data to write for {sample_file}')
            return

        # Convert the parsed data (list of dictionaries) into a DataFrame
        df = pd.DataFrame(data)

        # Save the DataFrame to a .txt file
        df.to_csv(sample_file, sep='\t', index=False)

        # Display the DataFrame to the user
        print(df)

        logger.info(f'Sample output written to {sample_file}')
    
    except Exception as e:
        logger.error(f'Error writing sample output: {e}')

def confirm_and_proceed():
    user_input = input('Review the sample data. Proceed with database insertion? (y/n): ')
    if user_input.lower() != 'y':
        logger.info('Operation cancelled by user.')
        sys.exit()

def setup_logging(log_file_path):
    logging.basicConfig(
        filename=log_file_path,
        filemode='a',  # Append to log file
        format='%(asctime)s - %(levelname)s - %(message)s',
        level=logging.INFO
    )
</file>

</repository_files>
